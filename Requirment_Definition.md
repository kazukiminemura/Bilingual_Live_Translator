# 要件定義書（改訂版）
**アプリ名（仮称）：** Bilingual Live Translator

---

## 1. 背景・目的
- 国際的な会議、学習、プレゼンテーションの場において、言語の壁を低減するために、
  **英語⇔日本語のリアルタイム音声翻訳アプリ**を開発する。  
- 音声を即時に文字化し翻訳、画面にわかりやすく表示することで、利用者の理解をサポートする。  

---

## 2. 利用シナリオ
1. 会議や講演中に、発話を即時翻訳して字幕として表示。  
2. 日本語スピーチを英語字幕に変換し、外国人参加者に提示。  
3. 語学学習者がリアルタイムで「原文＋翻訳文」を色分け表示し、理解を深める。  

---

## 3. 機能要件

### 3.1 音声入力
- マイク入力によるリアルタイム処理。  
- 音声方向（英語→日本語、日本語→英語）をユーザーが選択可能。  

### 3.2 音声認識（Speech-to-Text）
- 英語音声 → 英語テキスト化。  
- 日本語音声 → 日本語テキスト化。  
- 発話単位ごとに自動で区切りを検出。  

### 3.3 翻訳
- 英語 → 日本語翻訳。  
- 日本語 → 英語翻訳。  
- 発話単位ごとにリアルタイム変換。  

### 3.4 表示
- **リアルタイム字幕表示ウィンドウ**を提供。  
- 翻訳結果を即時に表示（遅延1～3秒程度）。  
- **追加機能**  
  - 英語音声のとき：「英語テキスト＋日本語翻訳」を同時表示。  
  - 日本語音声のとき：「日本語テキスト＋英語翻訳」を同時表示。  
  - **色分け表示**（例：原文＝青、翻訳＝オレンジ）。  
  - フォントサイズや背景色をユーザー設定可能。  

### 3.5 保存・出力
- 翻訳履歴をテキスト（.txt, .srt）に保存。  
- ログを会議記録や学習教材として活用可能。  

---

## 4. 非機能要件
- **リアルタイム性**：認識～翻訳～表示まで 3 秒以内。  
- **安定性**：長時間利用（1～2時間）でもクラッシュしない。  
- **UI/UX**：シンプルで直感的な操作（言語切替ボタン、表示ON/OFF）。  
- **クロスプラットフォーム**：PC（Windows/Mac）、モバイル（iOS/Android）。  

---

## 5. 技術要件
- **音声認識**：OpenAI Whisper / Google Speech-to-Text / Azure Speech SDK。  
- **翻訳**：OpenAI GPT-4o / DeepL API / Google Translate API。  
- **フロントエンド**：Electron（PC向け）または Flutter（クロスプラットフォーム）。  
- **バックエンド**：Python (FastAPI, Flask) または Node.js。  
- **表示更新**：WebSocketによるリアルタイム更新。  
- **UI**：React / Flutterで字幕ウィンドウ構築。  

---

## 6. 成果物
- プロトタイプアプリ（リアルタイム翻訳＋色分け表示対応）。  
- デモ動画（英語→日本語、日本語→英語両方）。  
- ユーザーマニュアル。  

---

## 7. 想定課題
- **精度**：雑音環境や専門用語で誤認識の可能性。  
- **翻訳のニュアンス**：直訳すぎる場合や文脈不足。  
- **パフォーマンス**：スマホ利用時の処理負荷・電池消費。  

---
