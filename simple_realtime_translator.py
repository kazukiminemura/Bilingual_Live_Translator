import asyncio
import sounddevice as sd
import numpy as np
from faster_whisper import WhisperModel
from transformers import pipeline

# Whisperãƒ¢ãƒ‡ãƒ« (å°ã•ã‚: "tiny", "small", "medium" ãªã©é¸æŠå¯)
asr_model = WhisperModel("medium", device="cpu")  # GPUãŒã‚ã‚Œã° "cuda"

# ç¿»è¨³ãƒ¢ãƒ‡ãƒ« (è‹±èªâ†’æ—¥æœ¬èª)
#translator = pipeline("translation", model="Helsinki-NLP/opus-mt-en-jap")
translator = pipeline("translation", model="Helsinki-NLP/opus-mt-jap-en")

# éŸ³å£°è¨­å®š
SAMPLE_RATE = 16000
BLOCK_DURATION = 3  # ç§’ã”ã¨ã«å‡¦ç†ï¼ˆçŸ­ãã™ã‚Œã°ç´°ã‹ãç¿»è¨³ã•ã‚Œã‚‹ï¼‰
BLOCK_SIZE = SAMPLE_RATE * BLOCK_DURATION

def process_audio(audio_chunk: np.ndarray):
    """1ãƒ–ãƒ­ãƒƒã‚¯åˆ†ã®éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–ï¼‹ç¿»è¨³ã—ã¦è¡¨ç¤º"""
    segments, _ = asr_model.transcribe(audio_chunk, beam_size=5)
    text = " ".join([seg.text for seg in segments]).strip()

    if text:
        # ç¿»è¨³
        translated = translator(text)[0]["translation_text"]
        print(f"[éŸ³å£°] {text}")
        print(f"[ç¿»è¨³] {translated}")
        print("-" * 40)

def audio_callback(indata, frames, time, status):
    if status:
        print("âš ï¸", status)
    audio_chunk = np.copy(indata[:, 0])  # ãƒ¢ãƒãƒ©ãƒ«ã§å–å¾—
    process_audio(audio_chunk)

async def main():
    print("ğŸ¤ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç¿»è¨³é–‹å§‹ (Ctrl+Cã§çµ‚äº†)")
    with sd.InputStream(callback=audio_callback,
                        channels=1,
                        samplerate=SAMPLE_RATE,
                        blocksize=BLOCK_SIZE,
                        dtype=np.float32):
        while True:
            await asyncio.sleep(0.1)

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ›‘ çµ‚äº†ã—ã¾ã—ãŸ")
